{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN CIFAR10 classifier with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.3.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/keras/tensorflow/keras_experiments/cifar10\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0\n",
      "Eager execution is: True\n",
      "Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version: {}\".format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10\n",
    "The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "\n",
    "\n",
    "class_names = \\[\n",
    "    'airplane', \n",
    "    'automobile', \n",
    "    'bird', \n",
    "    'cat', \n",
    "    'deer', \n",
    "    'dog', \n",
    "    'frog', \n",
    "    'horse', \n",
    "    'ship', \n",
    "    'truck'\n",
    "\\]\n",
    "\n",
    "<img src=\"images/cifar10.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" \n",
    "     width=850/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 32\n",
    "IMAGE_HEIGHT = 32\n",
    "NUM_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cifar10_image(image, width, height, channels):\n",
    "    reshaped = image.reshape(width, height, channels)\n",
    "    plt.figure(figsize=(1,1)),\n",
    "    plt.imshow(reshaped, cmap=plt.cm.binary)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_cifar10_images(instances, width=IMAGE_WIDTH, height=IMAGE_HEIGHT, channels=NUM_CHANNELS, images_per_row=5, **options):\n",
    "    plt.figure(figsize=(6,6))\n",
    "\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(width, height, channels) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((width, width * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data based on channels first / channels last strategy.\n",
    "# This is dependent on whether you use TF, Theano or CNTK as backend.\n",
    "# Source: https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "else:\n",
    "    input_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "NUMBER_OF_CLASSES = len(set(flatten(y_train)))\n",
    "NUMBER_OF_CLASSES = 10\n",
    "print(\"NUMBER_OF_CLASSES\", NUMBER_OF_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_cifar10_image(x_train[0], IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cifar10_images(x_train[0:25], IMAGE_WIDTH, IMAGE_HEIGHT, NUM_CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation/Shuffle\n",
    "indexes = np.arange(x_train.shape[0])\n",
    "indexes = np.random.permutation(indexes)  # shuffle data to randomly select\n",
    "x_train = x_train[indexes]\n",
    "y_train = y_train[indexes] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras normalization\n",
    "#x_full = np.concatenate((x_train, x_test), axis=0)\n",
    "#normalization = Normalization()\n",
    "#normalization.adapt(x_full)\n",
    "#x_train = normalization(x_train / 1.0).numpy()\n",
    "#x_test = normalization(x_test / 1.0).numpy()\n",
    "\n",
    "   \n",
    "\n",
    "# Cast\n",
    "#x_train, x_test = tf.cast(x_train, tf.float32), tf.cast(x_test, tf.float32)\n",
    "#y_train, y_test = tf.cast(y_train, tf.uint8), tf.cast(y_test, tf.uint8)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = x_train[:40000], x_train[40000:]\n",
    "y_train, y_valid = y_train[:40000], y_train[40000:]\n",
    "\n",
    "print(x_train[0][1])\n",
    "print(x_train[0].shape)\n",
    "print(x_train.shape)\n",
    "print(x_train.dtype)\n",
    "print(y_train.shape)\n",
    "print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data argumentation\n",
    "Shift, zoom, rotate, shear, flip the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# DO NOT normalize/standarlize data itself, or need to do the same to the test/validation data\n",
    "generator = ImageDataGenerator(\n",
    "    featurewise_center=True,              # Set input mean to 0 over the dataset, feature-wise. \n",
    "    featurewise_std_normalization=True,   # Divide inputs by std of the dataset, feature-wise.\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\"\"\"\n",
    "generator = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # fraction of total width if < 1 or pixels if >= 1\n",
    "    height_shift_range=0.1, # fraction of total height if < 1 or pixels if >= 1\n",
    "    shear_range=5,          # displaces each point in fixed direction with shear angle in counter-clockwise direction in degrees\n",
    "    zoom_range=(0.9, 1.1),  # single float zoom_range ([1-zoom_range, 1+zoom_range]) or [lower, upper].\n",
    "    rotation_range=10        # Int. degree range for random rotations.\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=True, \n",
    "    fill_mode='nearest',   # Points outside the boundaries of the input are filled according to the given mode. {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
    "    # cval=0                 # Float or Int. Value used for points outside the boundaries when fill_mode = \"constant\".\n",
    ")\n",
    "# Not necessary as no normalization/standadization.\n",
    "# generator.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "* C: Convolution layer\n",
    "* P: Pooling layer\n",
    "* B: Batch normalization layer\n",
    "* F: Fully connected layer\n",
    "* O: Output fully connected softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUMBER_OF_EPOCHS = 200\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "use_multiprocessing = True\n",
    "workers = 4\n",
    "\n",
    "DROPOUT_RATE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=/full_path_to_your_logs\n",
    "\n",
    "import os\n",
    "log_dir = os.getcwd() + os.path.sep + \"logs\"\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    histogram_freq=1,  # How often to log histogram visualizations\n",
    "    embeddings_freq=1,  # How often to log embedding visualizations\n",
    "    update_freq=\"epoch\",\n",
    ")  # How often to write logs (default: once per epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stop callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    #monitor='accuracy',\n",
    "    #monitor='val_loss',\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    mode='auto',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Traing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss_lambda = 0.010\n",
    "kernel_regularizer = regularizers.l2(l2_loss_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y):\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        generator.flow(x, y, batch_size=BATCH_SIZE),\n",
    "        steps_per_epoch=len(x_train) / BATCH_SIZE, \n",
    "        epochs=NUMBER_OF_EPOCHS,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        use_multiprocessing=use_multiprocessing,\n",
    "        workers=workers,\n",
    "        verbose=verbosity,\n",
    "        callbacks=[\n",
    "#            tensorboard_callback,\n",
    "            earlystop_callback\n",
    "        ]\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CPBFO model  (F/ReLU/He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #normalization,\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(\n",
    "        name=\"conv01\",\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(\n",
    "        name=\"pool01\",\n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(\n",
    "        name=\"batch_before_full01\"\n",
    "    ),\n",
    "    Dense(\n",
    "        name=\"full01\", \n",
    "        units=300, \n",
    "        activation=\"relu\"\n",
    "    ),     # Fully connected layer \n",
    "    Dense(\n",
    "        name=\"output_softmax\", \n",
    "        units=NUMBER_OF_CLASSES, \n",
    "        activation=\"softmax\"\n",
    "    )\n",
    "])\n",
    "\n",
    "history = train(model, x_train, y_train)\n",
    "print(history)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #normalization,\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(\n",
    "        name=\"conv01\",\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(\n",
    "        name=\"pool01\",\n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(\n",
    "        name=\"batch_before_full01\"\n",
    "    ),\n",
    "    Dense(\n",
    "        name=\"full01\", \n",
    "        units=300, \n",
    "        activation=\"relu\"\n",
    "    ),     # Fully connected layer \n",
    "    Dense(\n",
    "        name=\"output_softmax\", \n",
    "        units=NUMBER_OF_CLASSES, \n",
    "        activation=\"softmax\"\n",
    "    )\n",
    "])\n",
    "\n",
    "history = train(model, x_train, y_train)\n",
    "print(history)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CPBCPBFO model (F/ReLU/He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#    normalization,\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(\n",
    "        name=\"conv_01\",\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation=\"relu\"),                    # Fully connected layer \n",
    "    Dense(NUMBER_OF_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "model.layers\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CPBCPBFBFO model (F/ReLU/He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#    normalization,\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation=\"relu\"),                    # Fully connected layer \n",
    "    BatchNormalization(),\n",
    "    Dense(200, activation=\"relu\"),                    # Fully connected layer \n",
    "    Dense(NUMBER_OF_CLASSES, activation=\"softmax\")    # Output layer\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CPBCPBCPBFBFO model (F/ReLU/He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#    normalization,\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation=\"relu\"),                    # Fully connected layer \n",
    "    BatchNormalization(),\n",
    "    Dense(200, activation=\"relu\"),                    # Fully connected layer \n",
    "    Dense(NUMBER_OF_CLASSES, activation=\"softmax\")    # Output layer\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation=\"relu\"),                    # Fully connected layer \n",
    "    Dropout(DROPOUT_RATE),\n",
    "\n",
    "    BatchNormalization(),\n",
    "    Dense(200, activation=\"relu\"),                    # Fully connected layer \n",
    "    Dense(NUMBER_OF_CLASSES, activation=\"softmax\")    # Output layer\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NCPBCP(BC)+P(BF)+O model (F/ReLU/He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#    normalization,\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape,\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(\n",
    "        units=300, \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        activation=\"relu\"\n",
    "    ),                    # Fully connected layer \n",
    "    BatchNormalization(),\n",
    "    Dense(\n",
    "        units=200, \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        activation=\"relu\"\n",
    "    ),                    # Fully connected layer \n",
    "    Dense(NUMBER_OF_CLASSES, activation=\"softmax\")    # Output layer\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NCPBCP(BC)+P(BF)+O model (F/ReLU/He) + Drop outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#    normalization,\n",
    "    BatchNormalization(\n",
    "        name=\"input_normalization\"\n",
    "    ),\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape,\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "\n",
    "    ),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=128, \n",
    "        kernel_size=(3, 3), \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        activation='relu', \n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(\n",
    "        units=300, \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        activation=\"relu\"\n",
    "    ),                    # Fully connected layer \n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    Dense(\n",
    "        units=200, \n",
    "        kernel_regularizer=kernel_regularizer,\n",
    "        activation=\"relu\"\n",
    "    ),                    # Fully connected layer \n",
    "    \n",
    "    Dense(NUMBER_OF_CLASSES, activation=\"softmax\")    # Output layer\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CPBFO model  (F/SELU/LeCun)\n",
    "SELU activation + LeCun initialization at Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    normalization,\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        kernel_initializer='he_normal', \n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(\n",
    "        300, \n",
    "        kernel_initializer='lecun_normal', \n",
    "        activation='selu'\n",
    "    ),                    # Fully connected layer \n",
    "    Dense(\n",
    "        NUMBER_OF_CLASSES, \n",
    "        activation=\"softmax\"\n",
    "    )\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CPBCPBFO model (F/SELU/LeCun)\n",
    "SELU activation + LeCun initialization at Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    normalization,\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        kernel_initializer='he_normal', \n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        kernel_initializer='he_normal', \n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(\n",
    "        300, \n",
    "        kernel_initializer='lecun_normal', \n",
    "        activation='selu'\n",
    "    ),                    # Fully connected layer \n",
    "    Dense(\n",
    "        NUMBER_OF_CLASSES, \n",
    "        activation=\"softmax\"\n",
    "    )\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CPBCPBFFO model  (F/SELU/LeCun)\n",
    "SELU activation + LeCun initialization at Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    normalization,\n",
    "    Conv2D(                                           \n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        kernel_initializer='he_normal', # Using SELU+LeCun damages the validation\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(                                           \n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding=\"same\",\n",
    "        kernel_initializer='he_normal', \n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ),\n",
    "    MaxPooling2D(                                     \n",
    "        pool_size=(2, 2)\n",
    "    ),\n",
    "    Flatten(),                                        # 3D shape to 1D.\n",
    "    BatchNormalization(),\n",
    "    Dense(\n",
    "        300, \n",
    "        kernel_initializer='lecun_normal', \n",
    "        activation='selu'\n",
    "    ),                    # Fully connected layer \n",
    "    Dense(\n",
    "        200, \n",
    "        kernel_initializer='lecun_normal', \n",
    "        activation='selu'\n",
    "    ),                    # Fully connected layer \n",
    "    Dense(\n",
    "        NUMBER_OF_CLASSES, \n",
    "        activation=\"softmax\"\n",
    "    )\n",
    "])\n",
    "\n",
    "train(model, x_train, y_train)\n",
    "results = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"test loss, test accuracy:\", results)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.1-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
